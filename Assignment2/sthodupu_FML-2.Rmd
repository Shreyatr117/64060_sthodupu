---
title: "FML Assignment2"
author: "Shreya Thodupunuri 811301506"
date: "2023-09-29"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



```{r}
#importing the required packages 
library('caret')
library('e1071')
library('ISLR')
library('class')
```


```{r}
universal.df <- read.csv("~/Shreya R documents/UniversalBank.csv")
dim(universal.df)
t(t(names(universal.df))) 
```



```{r}
#Drop ID and ZIP
universal.df <- universal.df[,-c(1,5)]
```

```{r}
# Converting Education to a factor
universal.df$Education <- as.factor(universal.df$Education)

```


```{r}
#convert education to dummy variables
groups <- dummyVars(~., data = universal.df) # This creates the dummy groups
universal_m.df <- as.data.frame(predict(groups,universal.df))
```

```{r}
set.seed(1) 
train.index <- sample(row.names(universal_m.df), 0.6*dim(universal_m.df)[1])
valid.index <- setdiff(row.names(universal_m.df), train.index)  
train.df <- universal_m.df[train.index,]
valid.df <- universal_m.df[valid.index,]
t(t(names(train.df)))

```
```{r}
library(caTools)
set.seed(1)
split <- sample.split(universal_m.df, SplitRatio = 0.6)
training_set <- subset(universal_m.df, split == TRUE)
validation_set <- subset(universal_m.df, split == FALSE)
```

```{r}
# Print the sizes of the training and validation sets
print(paste("The size of the training set is:", nrow(training_set)))
print(paste("The size of the validation set is:", nrow(validation_set)))
```

```{r}
#Now, let us normalize the data
train.norm.df <- train.df[,-10] # Note that Personal Income is the 10th variable
valid.norm.df <- valid.df[,-10]

norm.values <- preProcess(train.df[, -10], method=c("center", "scale"))
train.norm.df <- predict(norm.values, train.df[, -10])
valid.norm.df <- predict(norm.values, valid.df[, -10])
```

### Questions

Consider the following customer:

1. Age = 40, Experience = 10, Income = 84, Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0, Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1, and Credit Card = 1. Perform a k-NN classification with all predictors except ID and ZIP code using k = 1. Remember to transform categorical predictors with more than two categories into dummy variables first. Specify the success class as 1 (loan acceptance), and use the default cutoff value of 0.5. How would this customer be classified?

```{r}
# We have converted all categorical variables to dummy variables
# Let's create a new sample
new_customer <- data.frame(
  Age = 40,
  Experience = 10,
  Income = 84,
  Family = 2,
  CCAvg = 2,
  Education.1 = 0,
  Education.2 = 1,
  Education.3 = 0,
  Mortgage = 0,
  Securities.Account = 0,
  CD.Account = 0,
  Online = 1,
  CreditCard = 1
)
```
```{r}
# Normalize the new customer
new.cust.norm <- new_customer
new.cust.norm <- predict(norm.values, new.cust.norm)
```


```{r}
#Now, let us predict using knn
knn.pred1 <- class::knn(train = train.norm.df, 
                       test = new.cust.norm, 
                       cl = train.df$Personal.Loan, k = 1)
knn.pred1

```

***

2. What is a choice of k that balances between overfitting and ignoring the predictor
information?

```{r}
# Calculate the accuracy for each value of k
# Setting the range of k values to be considered

accuracy.df <- data.frame(k = seq(1, 15, 1), overallaccuracy = rep(0, 15))
for(i in 1:15) {
  knn.pred <- class::knn(train = train.norm.df, 
                         test = valid.norm.df, 
                         cl = train.df$Personal.Loan, k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, 
                                       as.factor(valid.df$Personal.Loan),positive = "1")$overall[1]
}

which(accuracy.df[,2] == max(accuracy.df[,2])) 


plot(accuracy.df$k,accuracy.df$overallaccuracy)
#The best performing k in the range of 1 to 15 is 3.This k balances overfitting and ignoring predictions, and is the most accurate for 3.


```

3. Show the confusion matrix for the validation data that results from using the best k.
```{r}
pred <- class::knn(train=train.norm.df,
                   test=valid.norm.df,
                   cl=train.df$Personal.Loan,k=3)
confusionMatrix(pred,as.factor(valid.df$Personal.Loan))
```


4. Consider the following customer: Age = 40, Experience = 10, Income = 84,
Family = 2, CCAvg = 2, Education_1 = 0, Education_2 = 1, Education_3 = 0,
Mortgage = 0, Securities Account = 0, CD Account = 0, Online = 1 and Credit
Card = 1. Classify the customer using the best k.

```{r}
customer2.df <- data.frame(
Age = 40,
Experience = 10,
Income = 84,
Family = 2,
CCAvg = 2,
Education.1 = 0,
Education.2 = 1,
Education.3 = 0,
Mortgage = 0,
Securities.Account = 0,
CD.Account = 0,
Online = 1,
CreditCard = 1)
```
```{r}
# Normalizing the 2nd customer dataset
custnorm2 <- predict(norm.values,customer2.df)

```

5.Repeating the process by partitioning the data into three parts -
50%, 30%, 20%,Apply the k-NN method with the k chosen above. Compare the
confusion matrix of the test set with that of the training and validation sets.
Comment on the differences and their reason.
```{r}
# Split the data into training (50%), validation (30%), and test (20%) sets
set.seed(123)
Train.Index <- sample(row.names(universal_m.df), .5*dim(universal_m.df)[1])#create train index

```
```{r}
#creating validation index
Valid.Index <- sample(setdiff(row.names(universal_m.df),Train.Index),.3*dim(universal_m.df)[1])
Test.Index =setdiff(row.names(universal_m.df),union(Train.Index,Valid.Index))#create test index
train.df <- universal_m.df[Train.Index,]
cat("The size of the new training dataset is:", nrow(train.df))
```


```{r}
valid.df <- universal_m.df[Valid.Index, ]
cat("The size of the new validation dataset is:", nrow(valid.df))
```
```{r}
test.df <- universal_m.df[Test.Index,]
cat("The size of the new test dataset is:",nrow(test.df))

```

```{r}
#Data Normalizing
norm.values <- preProcess(train.df[, -10], method=c("center", "scale"))
train.df.norm <- predict(norm.values, train.df[, -10])
valid.df.norm <- predict(norm.values, valid.df[, -10])
test.df.norm <- predict(norm.values, test.df[,-10])

```

#Performing kNN and creating confusion matrix on training, testing, validation data sets
```{r}

pred3 <- class::knn(train = train.df.norm,
test = test.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(pred3,as.factor(test.df$Personal.Loan))
```

```{r}

pred4 <- class::knn(train = train.df.norm,
test = valid.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(pred4,as.factor(valid.df$Personal.Loan))
```

```{r}
pred5 <- class::knn(train = train.df.norm,
test = train.df.norm,
cl = train.df$Personal.Loan, k=3)
confusionMatrix(pred5,as.factor(train.df$Personal.Loan))
```

The sets are not mutually exclusive.